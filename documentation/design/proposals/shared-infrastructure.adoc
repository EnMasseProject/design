== Overview

Address spaces in EnMasse are currently isolated, i.e. they don't share infrastructure (router network, brokers). Moreover, the standard-controller and agent deployments are created per address space. Sharing infrastructure allows sharing resources and reducing the footprint when configuring multiple address spaces, and speed up the time to provision new address spaces. Furthermore, the split between `standard` and `brokered` address space types has caused a lot of quirks in the architecture, which a shared address space type may handle in a better way.

See the initial discussion in issue #3420 for more details on alternatives.

Another part of EnMasse that has been a source of confusion is Address Spaces and Namespaces. Earlier requirements around the service catalog integration led to supporting multiple AddressSpace instances per namespace. However, with CRDs, and the type of clusters EnMasse is deployed to, putting a restriction on one address space per namespace is a direction we want to go in. Doing this as part of shared infra is a good fit, as one would need to write the controller logic separately from existing code anyway.

Finally, the new design will take into account the service operator persona, which may be different from the service admin. The service operator may create plans, authentication services that is used by the tenants.

This document will consider the alternative with a new address space type + an alternative breaking API.

== Design overview

The new API will consist of the following types at the infrastructure level:

* `MessagingInfra`: used to define an instance of shared infrastructure. The `MessagingInfra` will define everything related to router and broker configuration that applies to all tenants. Routers and brokers will be created and owned by this resource in the same namespace as MessagingInfra is defined.

* `MessagingPlan`: Conceptually replaces the `AddressSpacePlan`, but can be optional or enforced. Is not referencing `AddressPlan`s.

* `AddressPlan`: Defines resources requested for an address using this plan. Referencing an AddressPlan may be optional but can be enforced. Resources are specified in terms of queue memory usage and depth, number of paritions, and type-specific settings such as DLQ and expiry queue.

* `AuthenticationService`: Gaining ability to use selectors to configure to which namespaces it should be applied.


At the tenant level, the API will change to simplify the user experience:

A restriction of 1 instance of messaging per Kubernetes namespace is mandated. New CRDs for configuring messaging endpoints, connectors and plan/authservice config is defined. The AddressSpace resource is replaced by `MessagingTenant`, `MessagingEndpoint` and `MessagingConnector` resources:

* `MessagingTenant`: a singleton per namespace, is introduced to contain the messaging configuration of a namespace. Auto-created when initial `Address` is created. May be used to override the default `MessagingPlan` and `AuthenticationService` of a tenant.

* `MessagingConnector`: similar to existing connector spec in AddressSpace. 1 instance configures 1 connector.

* `MessagingEndpoint`: allows configuring endpoints to access messaging infrastructure. Cluster-local endpoint is created by default on initial `Address` creation.

* `Address`: similar to existing type. Name is no longer prefixed, `.spec.address` is optional. `.spec.plan` is optional if permitted by plan config. Addition per-type sub-objects such as `.spec.queue` or `.spec.topic` may be specified to override plan properties (if allowed by plan).

* `MessagingRole` and `MessagingRoleBinding`: manage authentication and authorization. Not defined in this proposal and is orthogonal to the use of shared infra.


The `MessagingPlan`, `AddressPlan` and `AuthenticationService` resources may additionally be referenced by their namespace. This allows having a service operator persona that does not manage or have access to the messaging infrastructure, but manages the plans and authentication service configurations offered to messaging tenants.

=== Service operator

The service operator persona is introduced to allow managing messaging plans, address plans, and authentication services separately from the rest of main infrastructure. Any reference to plans and authentication services will require using an object with name and namespace referencing the resource.

=== Operations

With shared infrastructure, monitoring and alerting of routers and brokers is requires as today, but the visibility into the per-namespace 'slice' of the infrastructure is also needed to debug and observe the system. The service admin/operator need the ability to:

* Locate routers and brokers serving endpoints of a namespace
* Locate broker(s) for a given address
* Inspect the current version of a shared infrastructure instance
* Determine requested and actual resource usage of a namespace
* Determine placement of routers/brokers to ensure proper HA configuration

This should be available in console, APIs and through a command line tool.

=== Tenants

With shared infrastructure, tenants will share resources with other tenants, and it is critical that tenants can observe statistics for their 'share'. These statistics include:

* Address, connection and link statistics (covered by console API?)
* MessagingTenant status (CRD)
* MessagingEndpoint status (CRD)

=== IoT

IoT design will be covered in a separate proposal.

=== Resources

==== MessagingInfra

The MessagingInfra describes an instance of shared infrastructure managed by the enmasse operator.

Example:

```
apiVersion: enmasse.io/v1
kind: MessagingInfra
metadata:
  name: infra1
spec:
  # Allows specifying a selector for namespaces that should use this infra by default.
  namespaceSelector:
    matchLabels:
      type: messaging
  router:
    image: # Optional. Allows overriding router image

    minReady: 1 # <- PodDisruptionBudgets

    scalingStrategy:
      # Either a static scale strategy that does not scale!
      static:
        replicas: 4 
      # Or one that scales on the number of tenants?
      vhost:
        replicasRange:
          min: 2
          max: 4
        # Create another router when we exceed this many vhosts
        tenantsPerRouter: 100
      # Or one that scales by the number of connections?
      connection:
        replicasRange:
          min: 2
          max: 4
         # Create another when we have more than 100 connections per router
        connectionsPerRouter: 100
    podTemplate: # Same as in standard infra config today. Based on Kubernetes spec
      spec:
        affinity: # Allow explicitly setting affinity rules to enforce specific nodes
        tolerations: # Allow enforcing which nodes to run on
        resources: # Memory and CPU settings
        priorityClassName: # Pod priority settings

    ... # Other router settings - same as in StandardInfraConfig, except vhost policies (which are derived per address space plan)

  broker:
    image: # Optional. Allows overriding broker image

    # Determines how we scale brokers
    scalingStrategy:
      # Either fixed number of brokers
      static:
        poolSize: 3
      # Or one that scales by the number of queue usage
      addressUsage:
        # I.e. create another broker when we exceed this amount of memory used for addresses
        memoryUsagePerBroker: 100Mi 
        poolSizeRange:
          min: 0
          max: 5

    # For HA. Must be >= 1, defaults to 1. Larger values creates HA replicas that are spread using anti-affinity
    replicas: 1 

    addressFullPolicy: BLOCK
    storageClassName: # Storage class name to use for PVCs.
    podTemplate:
      metadata:
      spec:
        affinity:
        tolerations:
        resources:
        priorityClassName:
status:
  phase: Active
  conditions:
  - type: RoutersCreated
    status: True
  - type: Ready
    status: True
```

The brokers in a pool may be configured according to the address spaces they need to support.

==== MessagingPlan

The `MessagingPlan` defines limits that map to a particular namespaces' share of the infrastructure:

For instance, routers per vhost policies:

* connections
* links per connection

To broker settings:

* transactional capability
* queue memory usage

To operator limits:

* number of addresses

A split into requests and limits similar to Kubernetes resources is used to allow the operator to calculate the cost of applying plans vs. infrastructure capacity when creating the plan, as well as the cost of applying the plan.

The `MessagingPlan` schema may look like the following:

```
apiVersion: enmasse.io/v1
kind: MessagingPlan
metadata:
  name: small
spec:
  # Allows specifying a selector for namespaces that should be configured using this plan.
  namespaceSelector: 
    matchLabels:
      type: messaging
  capabilities:
    - transactions
  resources:
    requests:
      queueMemory: 10Mi # This mount is shared among addresses on a broker for a particular namespace
      addresses: 10 # Mainly for sizing operator
      connections: 1 # Router limits
    limits:
      queueMemory: 100Mi
      addresses: 100
      connections: 10
      linksPerConnection: 2
```

This is easier to understand and reason about than fractions used in `AddressSpacePlan`s. It is also easier to relate the impact of those limits to the shared infra, and allow the limits to be enforced. It would allow a shared infra to support messaging plans with different limits in place. 

For routers and brokers, the shared infra has some potential for auto-scaling. It would be the responsibility of the enmasse-operator to scale the infrastructure within the bounds set by the `MessagingInfra`, which could be based on cpu and memory usage, or the limits defined in the messaging plans.

For plans with the `transactions` capability, the operator will ensure that addresses for that namespace are always link-routed and put on the same broker (and HA replicas).

NOTE: If no `MessagingPlan` instances are defined, the system will still work. However, there will be no limits configured for particular vhosts.

==== MessagingTenant

The messaging tenant will create a singleton instance of `MessagingTenant` in a namespace to
configure messaging. The MessagingTenant is created by the EnMasse operator when the initial Address
is created. If the MessagingTenant already exists, the infra, plan and authentication services that
are specified will be used _only if_ those have selectors for that namespace.

```
kind: MessagingTenant
metadata:
  name: default # Singletons can be enforced using openapi validation!
spec:
  infra:
    name: shared
    namespace: custom
  plan:
    name: myplan
    namespace: custom
  authenticationService:
    name: myservice
    namespace: custom
```

==== MessagingEndpoint

A MessagingEndpoint configures access to the messaging infrastructure via different mechanisms. It
is also the place where tenants can explicitly configure certificates to be used, or configure how an endpoint should be exposed.

An endpoint can either be exposed internally (as a ClusterIP service), as a loadbalancer service
(LoadBalancer service) or as an OpenShift route. Multiple endpoints may be created per namespace.

All addresses in a namespace are exposed through all endpoints.

```
kind: MessagingEndpoint
metadata:
  name: myendpoint
  namespace: myapp
spec:
  certificate:
    selfsigned: {} # Default
    openshift: {}
    provided:
      secret:
        name: mycert # Get cert from secret
        namespace: myapp
  
  # Only one of 'internal', 'route' and 'loadbalancer' may be specified for each endpoint
  internal: # Expose as a ClusterIP service for applications on cluster
    ports:
    - name: amqp
    - name: amqps
    - name: amqpws
      port: 8080 # Port is optional
    - name: amqpwss
      port: 443

  route: # Expose as route
    host: example.com
    ports:
    - name: amqps
    - name: amqpwss

  loadbalancer:
    annotations: {}
    ports:
    - name: amqp

  exports:
  - name: myconfig # Same as before?
    kind: ConfigMap
status:
  ca: # CA of endpoint (if selfsigned)
  host: myendpoint-myapp.enmasse-infra.svc # Host is based on name and namespace of endpoint
  ports:
  - name: amqp
    port: 5672
```

==== AddressPlan

Address plans allow properties to indicate the desired guarantees of a queue. An example address plan would be:

```
apiVersion: enmasse.io/v1
kind: AddressPlan
metadata:
  name: small-queue
spec:
  namespaceSelector:
    matchLabels:
     type: messaging
  resources:
    requests:
      queueMemory: 3Mi
    limits:
      queueMemory: 4Mi

  allowOverrides: true # true means addresses are allowed to override plan settings

  queue: # Settings related to queue types
    partitions: # Specifying a min and max allow the operator to make a decisions to split queue across multiple brokers to fit it. Setting max >= 1 may cause message affects message ordering
      min: 1
      max: 2
    ttl: 60s
    # Create these addresses on the same broker (requirements same as for this address)
    expiryQueue: exp1
    deadLetterQueue: dlq1
```

For instance, the resources spec will be used when scheduling the queue to ensure it is placed on a broker that meets the memory requirements for the queue.

Other properties are specified for each type. I.e. queue properties are under .spec.queue, topic properties are under .spec.topic etc.

Addresses allow setting the same properties as the plan, if permitted by the allowOverrides setting in the plan.

A note on partitions: the new scheduler should take broker topology in the shared infra into account when placing queues so that they are not put in brokers in the same AZ (if multiple are configured)

```
apiVersion: enmasse.io/v1
kind: Address
metadata:
  name: addr1
spec:
  address: addr1 # Optional. Defaults to .metadata.name
  type: queue
  plan: small-queue # Optional. Defaults to no limits and can be placed on any broker
  queue: # Queue settings overriding plan settings
    ttl: 1200s
```

The set of properties for a given address will drive the placement of that queue.

This can be translated to limits that can be enforced in the broker, and that can be reasoned about from a sizing perspective. Properties specified on an `Address` may also be specified on an `AddressPlan`, and the plan may restrict if properties can be overridden or not.

==== MessagingConnector

For phase 2, but to configure connectors:

```
kind: MessagingConnector
matadata:
  name: connector
spec:
  # Same options as under address space .spec.connectors[]
status:
  # Same options as under address space .spec.connectors[]
```

==== Other considerations and removed components

The following components will not be part of shared infra:

* MQTT Gateway
* MQTT LWT
* Subserv
* Address-space-controller
* Agent
* Standard-controller
* Topic-forwarder (The implication is that partitioned/sharded topics will not be supported - at least initially)

=== Phase 1 (Milestone 0.32.0 ?)

The goal of phase 1 is to add support for shared infra and implement basic features similar to standard address space.

The `MessagingInfra` resource would be managed by the enmasse-operator, which will do a reconciliation of deployments, services etc. The router-operator should be used to deploy and manage the dispatch router to simplify the interface. Depending on the maturity of the broker-operator, it should be used to deploy the brokers.

The `MessagingTenant` resource will be managed by a controller in enmasse-operator. The controller will create vhost policies in the shared router infra for each namespace with a config, and apply restrictions as specified in the messaging plan.

The `Address` resource for `shared` infra will be managed by a controller in enmasse-operator. The controller will watch all addresses across all namespaces, and apply the needed address configuration to brokers and routers using AMQP management.

After the first phase, the following would be supported:

* Deploy shared infra using the `MessagingInfra` resource
* Creating messaging configs and messaging endpoints in a namespace
* Anycast, multicast, queue, non-sharded topics supported, subscription (no transactions etc. yet)
* Management using console

The following would NOT be supported:

* Broker HA
* Non-mesh router topologies
* Features not supported by router or broker operators
* Connectors and forwarders
* Broker-semantics for addresses
* Configure per-address space limits
* Configure per-address limits
* MQTT, Core, STOMP

==== Detailed design

The tasks of managing brokers and routers should be offloaded to standalone components as much as possible.

For router deployments, the builtin mesh-forming support of the router image will be used. All router configuration except basic static config is applied using AMQP management, as the router does not have a mechanism to distinguish between static and dynamic configuration.

For broker deployments, a standard upstream broker image will be used (once provided, in the meantime use existing image). All broker configuration defined in MessagingInfra is applied statically to the broker.xml and makes use of the auto-reload feature of the broker config.

Router - Broker connections are maintained by having the operator create and maintain the router -> broker connectors. The advantage is that we no longer rely on custom plugin code for connections, and that we get more flexibility in choosing topology of connections (i.e. multiple routers can connect to the same broker for better HA). 

Performance goals of a shared infra instance:

* Handle up to 100k addresses - possibly spread accross multiple namespaces
* Handle up to 1000 namespaces per infra instance (with 100 addresses each)

Important design considerations:

* Minimize management traffic with router and broker
* Add safeguards for getting out of bad states (with proper error reporting to be able to investigate bugs later)
* Shared infra/MessagingInfra instances should be able to operate independently in isolation

===== Configuration

The configuration can be broken down into different lifecycle 'levels':

* Infra - configuration that is applied at all routers and brokers (based on the MessagingInfra config)
* Namespace - configuration that is applied for each namespace (based on MessagingTenant, MessagingPlan, MessagingEndpoint and MessagingConnector)
* Address - configuration that is applied for each address (based on Address and AddressPlan)

For the routers, each level will involve the following configuration:

* Infra: Connectors to broker. Global router settings such as threads, internal certs. Pod template settings. Changes modify the router using AMQP management.
* Namespace: Vhost policies, endpoints, external connectors. Changes are applied using AMQP management to avoid router restart.
* Address: Address waypoints, autolinks, linkroutes. Changes are applied using AMQP management to avoid router restart

For the brokers:

* Infra: Global broker settings such as JVM size, global max size, storage size, global policies. Changes modify the generated broker.xml and requires a broker restart.
* Address level: queues and topics, per address limits. Configured using AMQP management to avoid broker restart.

The operator will maintain open connections to all routers and brokers. The connection will be periodically closed to enforce a resync so that configuration does not drift.

Once the connection is open, the operator will retrieve the applied configuration for that component and maintain an internal state representation of that components configuration. Whenever new messaging configs or addresses are created or updated, the internal state will be changed, and changes applied to the router and broker.

Should the configuration of routers and brokers drift (i.e. by manual intervention or bugs), the periodic resync will correct the configuration.

===== Status checks

Routers will be periodically queried (by independent goroutines) for:

* Autolink states
* Linkroute states
* Connection states
* Link states

The data will be stored in memory available to the messaging config and address controller loops as well.

===== Controllers

The following controllers and components must be implemented:

* Messaging-infra controller - Managing the shared infra
** State representation model - Used by other controllers to apply configuration to shared infra
* Messaging-tenant controller - Managing messaging configuration of namespaces
* Address controller - Managing addresses of namespaces
** Address scheduler - Used for placing queues on a set of brokers with different properties/capabilities
* Messaging-endpoint controller - Managing messaging endpoints of namespaces
* (Phase 2) Messaging-connector controller - Managing external connectors of namespaces

=== Plan behavior

Both for MessagingTenantPlan and AddressPlans - Only MessagingTenantPlan used in examples here.

1. No plan - No plans defined by service admin - no plan specified by messaging tenant => no plan applied => no limits
2. Restricted - Plans defined by service admin - plan set by messaging tenant
3. Restricted by default - Plans defined and default defined by service admin - plan not set by messaging tenant

== Unlimited - No plans defined by service admin - no plan specified by messaging tenant => no limits

```
apiVersion: enmasse.io/v1
kind: MessagingInfra
metadata:
  name: infra1
spec: {}
```

```
apiVersion: enmasse.io/v1
kind: MessagingTenant
metadata:
  name: default
  namespace: myspace
spec: {}
```

== Restricted - Plans defined by service admin - plan set by messaging tenant

```
apiVersion: enmasse.io/v1
kind: MessagingInfra
metadata:
  name: infra1
spec: {}
```

```
apiVersion: enmasse.io/v1
kind: MessagingTenantPlan
metadata:
  name: plan1
  namespace: enmasse-plans
spec:
  // ...
```

```
apiVersion: enmasse.io/v1
kind: MessagingTenant
metadata:
  name: default
  namespace: myspace
spec:
  plan:
    name: plan1
    namespace: enmasse-plans
```

If plan1 is modified, changes are applied to all messaging tenants.

== Restricted by default - Plans defined by service admin and default is set - plan not set by messaging tenant

```
apiVersion: enmasse.io/v1
kind: MessagingInfra
metadata:
  name: infra1
spec: {}
```

```
apiVersion: enmasse.io/v1
kind: MessagingTenantPlan
metadata:
  name: plan1
  namespace: enmasse-plans
spec:
   // ...
apiVersion: enmasse.io/v1
kind: MessagingTenantPlan
metadata:
  name: plan2
  namespace: enmasse-plans
  annotations:
    enmasse.io/default: "true"
spec:
   // ...
```

```
apiVersion: enmasse.io/v1
kind: MessagingTenant
metadata:
  name: default
  namespace: myspace
spec: {}
```

Alternative 1: After initial reconcile, MessagingTenant is updated (plan is materialized):

```
apiVersion: enmasse.io/v1
kind: MessagingTenant
metadata:
  name: default
  namespace: myspace
spec:
  plan:
    name: plan2
    namespace: enmasse-plans
```

Alternative 2: After initial reconcile, MessagingTenant is not updated.

NOTE:
* Alternative 1: If default changes from `plan2` to `plan1`, nothing changes
* Alternative 2: If default changes from `plan2` to `plan1`, change all plans without anything set
* Both alternatives: If `plan2` is modified, changes are applied to all messaging tenants

==== Tasks

===== Task 1: Create new CRDs (small)

* Create the MessaginInfra, MessagingTenant, MessagingEndpoint CRD + OpenAPI.

==== Task 2: Implement messaging-infra controller in controller-manager (large)

The messaging-infra controller is responsible for managing router and broker deployments and ensure they have the configuration as requested in the config.

The controller should:

* Watch MessagingInfra CR
* Creates router statefulset to deploy routers based on infra config and using input from system metrics to adjust the number of replicas
* Creates broker statefulset to deploy brokers based on infra config and using input from system metrics to adjust the number of replicas
* Creates interal state representation for each router and broker in the CR status. This state should be shared with other controllers (details below)
* Expose metrics about connections and links which is used by the console, either through HTTP or AMQP

===== Internal state representation

A components state encapsulates the configuration state of a broker or router in memory. Whenever a router or broker is connected, a corresponding router/broker state object is initialized with configuration retrieved from querying the router/broker. If disconnected, the state object is initialized with current state, and desired state is applied.

The state object has methods to apply configuration (i.e. applyAddress, applyMessagingEndpoint, applyMessagingTenant etc.). These methods compare the actual configuration of the underlying component to the desired configuration (transformed into autolinks etc.). If the applied configuration is different to the internal state, the underlying component is updated using AMQP management.

In addition, each state object has a goroutine which periodically polls its underlying router/broker for all status information and caches it for use by controllers to update the status of their respective resources.

===== Certificate management

Communication between components in the shared infrastructure should be secured using TLS. The messaging-infra-controller needs to create a CA per messaging-infra instance, as well as handle certificate rotation for the internal certificates on expiry.

==== Task 3: Implement messaging-tenant controller in controller-manager (medium)

The messaging-tenant controller manages the MessagingTenant CR

* Watch MessagingTenant CRs
* Use label selectors of MessagingInfra to locate the infra for namespace
* Lookup infra state representation
* For each router:
** Apply vhost settings+limits, authentication service information
** Fetch latest known status and update CR status accordingly
* Expose metrics of connections and linke based on router status.
* Requeue for processing at configurable interval

==== Task 4: Implement address controller in controller-manager (medium)

* Watch Address CR
* if new address:
** Invoke queue scheduling to configure which brokers address should be placed on
* Find MessagingInfra where this is placed (unless it is 
* Lookup state objects for routers and brokers
* For each router:
** Apply autolinks, linkroutes and addresses
** Fetch latest known status and update address status
* For each broker:
** Apply autolinks, linkroutes and addresses
** Fetch latest known status and update address status
* (Optional phase 2): Expose address metrics based on status
* Requeue address for processing at configurable interval


==== Task 5: Implement queue/topic/subscription scheduling (medium)

The initial version of the queue scheduler should be similar to what we have in the standard address space. It should:

* Allow sharding queues across multiple brokers
* Place addresses on brokers that matches desired semantics
* (Optional phase 2): Take broker anti-affinity into consideration during placement
* (Optional phase 2): Take available broker memory for queue into account during placement

==== Task 6: Implement messaging-endpoint controller in controller-manager (medium)

* Watch MessagingEndpoint CR
* Find MessagingInfra where this is placed (based on referenced MessagingPlan)
* Lookup infra state representation
* Create corresponding service, route, loadbalancer service
* For each router:
** Apply endpoint configuration with certs
** Fetch latest known status and update CR status accordingly
* Expose metrics of endpoint
* Requeue for processing at configurable interval



=== Phase 2 (Milestone 0.33.0 ?)

The second phase will expand the supported features of the shared infra. The shared infra will gain support for deploying broker clusters and assign addresses requiring a broker cluster to them.

After the second phase, the following would be supported as well:

* Connectors and forwarders
* Configure per-address space limits
* Configure per-address limits
* Broker-semantics for addresses - allow 'transactional' address spaces
* Deprecate standard

The following would NOT be supported:
* MQTT, Core, STOMP

=== Phase 3 (Milestone 0.34.0 ?)

* The missing protocol support could be addressed in some way.
* Handle migration from `brokered` and `standard` to `shared`, potentially as part of the enmasse-operator
* Deprecate brokered

=== Phase 4 (Milestone 0.X.0 ?)

Phase 4 would mainly involve removing `brokered` and `standard`, once the oldest version supported in upgrades has deprecated brokered and standard.

* Remove brokered and standard address space types
* Removal of address space `type` field
* Removal of BrokeredInfraConfig and StandardInfraConfig CRDs

== Testing

A new class of tests for shared infra should be created. The address-space-specific tests should be able to reuse the infra to speed up testing. Some tests would still need to be written to test that one can run multiple shared infra instances.

A load-test is also essential to ensure that the operator can handle a large number of address spaces and addresses.

== Documentation

The shared address space will cause a lot of changes to the documentation, and it might be good to create a separate chapter for both service admin and messaging tenant related to shared infra specifically. 
