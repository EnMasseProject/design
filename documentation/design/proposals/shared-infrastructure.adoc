== Overview

Address spaces in EnMasse are currently isolated, i.e. they don't share infrastructure (router network, brokers). Moreover, the standard-controller and agent deployments are created per address space. Sharing infrastructure allows sharing resources and reducing the footprint when configuring multiple address spaces, and speed up the time to provision new address spaces. Furthermore, the split between `standard` and `brokered` address space types has caused a lot of quirks in the architecture, which a shared address space type may handle in a better way.

See the initial discussion in issue #3420 for more details on alternatives. This document will consider the alternative with a new address space type for shared infra.

== Design

A new address space type `shared` is introduced. This address space type will not be handled by the `address-space-controller`, which will have a filter to avoid processing such address spaces. Eventually, allow `type` field of an address space to be omitted, and make the absence of this field indicate that the `shared` type should be used.

The `AddressSpacePlan` will also have the 'shared' addressSpaceType. A new field `messagingInfraRef` can refer to a new CRD type instace, `MessagingInfra`. This new type will contain configuration for both brokers and router.

The infrastructure is deployed is when the `MessagingInfra` instance is created. Alternatively, it could be deployed on-demand when an address space referencing it is created. This behavior could potentially be configurable to give the service admins more control.

Example:

```
apiVersion: admin.enmasse.io/v1beta1
kind: MessagingInfra
metadata:
  name: infra1
spec:
  router:
    image: # Optional. Allows overriding router image

    replicas:
      min: 2 # Min replicas will be deployed when infra is created
      max: 3 # Max replicas that maybe scaled to
      minReady: 1 # Allows 1 router to not be ready before alerting

    topology: mesh | ring # Optional (default: mesh) How routers should be connected.

    # Optional (default: 1 entry with defaults set). Allows creating additional router groups, each with identical group configuration applied. Strict anti-affinity rules based on name will be applied to each group to enforce they are not running in the same zone
    groups: 
    - name: grp1
      linkWeight: 30 # Optional (default: same as internal weight). Allows controlling the weight of links to other router groups
    - name: grp2
      linkWeight: 60

    linkWeight: 100 # The weight of internal router links
    podTemplate: # Same as in standard infra config today. Based on Kubernetes spec
      spec:
        affinity: # Allow setting affinity rules to ensure routers do not run the same node etc.
        tolerations: # Allow enforcing which nodes to run on
        resources: # Memory and CPU settings
        priorityClassName: # Pod priority settings

    ... # Other router settings - same as in StandardInfraConfig, except vhost policies (which are derived per address space plan)

  brokers:
    image: # Optional. Allows overriding broker image
    min: 1 // Can be > 0 to pre-create brokers
    max: 5 // Will never create more brokers than this
    replicas: 1 // Must be >= 1, defaults to 1. Larger values creates HA replicas
    addressFullPolicy: BLOCK
    storageClassName: #

    podTemplate:
      metadata:
      spec:
        affinity:
        tolerations:
        resources:
        priorityClassName:
     
    ... # Other broker settings - ensure 
```

When deployed, the above would have the minimum impact of creating 3 routers and 1 brokers. The brokers may be configured according to the address spaces they need to support.

An `AddressSpacePlan` currently refers to router and broker credits to control the 'quota' of address spaces. With shared infra, the numbers would either have to change their meaning, or one could reithink and extend the schema.

For instance, address space limits can be expressed in more specific terms that map to the routers per vhost policies:

* connections
* links per connection

In addition, operator limits such as number of addresses can be specified:

* number of addresses

Moreover, a split into requests and limits similar to Kubernetes resources is used to allow the operator to calculate the cost of applying plans vs. infrastructure capacity when creating the plan, as well as the cost of applying the plan.

The `AddressSpacePlan` would then look like this:

```
apiVersion: admin.enmasse.io/v1beta2
kind: AddressSpacePlan
metadata:
  name: shared-small
spec:
  messagingInfraRef: shared1 // Plans may refer to the same infra - this means address spaces will run on the same infra
  capabilities:
    - transactions
  resources:
    requests:
      queueMemory: 10Mi # This mount is shared among addresses on a broker for a particular address space
      addresses: 10 # Mainly for sizing operator
      connections: 1 # Router limits
    limits:
      queueMemory: 100Mi
      addresses: 100
      connections: 10
      linksPerConnection: 2
```

This would be easier to understand and reason about than fractions. It is also easier to relate the impact of those limits to the shared infra, and allow the limits to be enforced. It would allow a shared infra to support address space plans with different limits in place. 

NOTE: Plans for existing address space types would continue to use the existing schema.

For routers and brokers, the shared infra has some potential for auto-scaling. It would be the responsibility of the enmasse-operator to scale the infrastructure within the bounds set by the `MessagingInfra`, which could be based on cpu and memory usage, or the limits defined in the address space plans.

Address spaces may also contain capabilities to indicate what sort of broker that needs to be used for the addresses:

```
apiVersion: enmasse.io/v1beta1
kind: AddressSpace
metadata:
  name: myspace
spec:
  type: shared
  capabilities:
    - transactions
```

The operator will ensure that addresses for this address space is always link-routed and put on the same broker (and HA replicas).

Address plans allow properties to indicate the desired guarantees of a queue. An example address plan would be:

```
apiVersion: admin.enmasse.io/v1beta2
kind: AddressPlan
metadata:
  name: small-queue
spec:
  addressType: queue
  resources:
    requests:
      queueDepth: 3
    limits:
      queueDepth: 10

  allowOverrides: true # true means addresses are allowed to override plan settings
  queue: # Settings related to queue types
    partitions: # Specifying a min and max allow the operator to make a decisions to split queue across multiple brokers to fit it. Setting max >= 1 may cause message affects message ordering
      min: 1
      max: 2
    ttl: 60s
    # Create these addresses on the same broker (requirements same as for this address)
    expiryQueue: exp1
    deadLetterQueue: dlq1
    
```

Addresses allow setting the same properties as the plan, if permitted:

```
apiVersion: enmasse.io/v1beta1
kind: Address
metadata:
  name: myspace.addr1
spec:
  address: addr1
  type: queue
  plan: small-queue
  queue: // Queue settings
    ttl: 1200s
```

The set of properties for a given address will drive the placement of that queue, either on a link-routed broker, or across a set of brokers.

This can be translated to limits that can be enforced in the broker, and that can be reasoned about from a sizing perspective. Properties specified on an `Address` may also be specified on an `AddressPlan`, and the plan may restrict if properties can be overridden or not.

NOTE: Plans for existing address types would continue to use the existing schema.

The following components will not be part of shared infra:

* MQTT Gateway
* MQTT LWT
* Subserv
* Address-space-controller
* Agent
* Standard-controller
* Topic-forwarder (The implication is that partitioned/sharded topics will not be supported - at least initially)

=== Phase 1 (Milestone 0.32.0 ?)

Add support for shared infra address space type and implement basic features similar to standard address space.

The `MessagingInfra` resource would be managed by the enmasse-operator, which will do a reconciliation of deployments, services etc. The router-operator should be used to deploy and manage the dispatch router to simplify the interface. Depending on the maturity of the broker-operator, it should be used to deploy the brokers.

The `AddressSpace` resource of type `shared` will be managed by a controller in enmasse-operator. The controller will create vhost policies in the shared router infra for each address space, and apply restrictions as specified in the address space plan.

The `Address` resource for `shared` address space types will be managed by a controller in enmasse-operator. The controller will watch all addresses across all address spaces, and apply the needed address configuration to brokers and routers using AMQP management.

After the first phase, the following would be supported:

* Deploy shared infra using the `MessagingInfra` resource
* Creating 1 or more address spaces per shared infra
* Anycast, multicast, queue and non-sharded topics supported (no transactions etc. yet)
* Management using AMQ console

The following would NOT be supported:

* Broker HA
* Non-mesh router topologies
* Features not supported by router or broker operators
* Connectors and forwarders
* Broker-semantics for addresses
* Configure per-address space limits
* Configure per-address limits
* MQTT, Core, STOMP

==== Design

The tasks of managing brokers and routers should be offloaded to standalone components as much as possible.

For router deployments, the https://github.com/interconnectedcloud/qdr-operator[qdr-operator] will be used and it supports all features required by AMQ Online.

For broker deployments, the https://github.com/rh-messaging/activemq-artemis-operator[activemq-artemis-operator] will be used, (and modified to work with our requirements). The resulting changes should be submitted upstream, although short term there may exist a temporary fork in order to make progress.

Router - Broker connections can be maintained either by having the operator create and maintain the router -> broker connectors, or use the broker-plugin to create connectors to the routers. The advantage of the first alternative is that we no longer rely on custom plugin code for connections, and that we get more flexibility in choosing topology of connections (i.e. multiple routers can connect to the same broker for better HA). The second alternative removes the need to create the outgoing router connector. However, we already need the capability to create connectors in the router (for external bridges), so this is not really something we get away with. For this design, having the operator maintain the connectors will be the preferred alternative.

Performance requirements of shared infra:

* Handle up to 100k addresses - possibly spread accross multiple address spaces
* Handle up to 1000 address spaces per infra instance (with 100 addresses each)

Improtant design considerations:

* Minimize management traffic with router and broker
* Add safeguards for getting out of bad states (with proper error reporting to be able to investigate bugs later)
* Shared infras should be able to operate independently

===== Configuration

The configuration can be broken down into different lifecycle 'levels':

* Infra - configuration that is applied at all routers and brokers (based on the MessagingInfra config)
* Address space - configuration that is applied for each address space (based on AddressSpace and AddressSpacePlan)
* Address - configuration that is applied for each address (based on Address and AddressPlan)

For the routers, each level will involve the following configuration:

* Infra: Connectors to broker. Global router settings such as threads, internal certs. Pod template settings. Changes modify the router CR and require a rolling router restart (until DISPATCH-1519 is implemented)
* Address space: Vhost policies, external connectors. Changes are applied using AMQP management to avoid router restart.
* Address: Address waypoints, autolinks, linkroutes. Changes are applied using AMQP management to avoid router restart

For the brokers:

* Infra: Global broker settings such as JVM size, global max size, storage size, global policies. Changes modify a Broker CR and requires a broker restart.
* Address level: queues and topics, per address limits. Configured using AMQP management to avoid broker restart.

The operator will maintain open connections to all routers and brokers. The connection will be periodically closed to enforce a resync so that configuration does not drift.

Once the connection is open, the operator will retrieve the applied configuration for that component and maintain an internal state representation of that components configuration. Whenever new address spaces or addresses are created or updated, the internal state will be changed, and changes applied to the router and broker.

Should the configuration of routers and brokers drift (i.e. by manual intervention or bugs), the periodic resync will correct the configuration.

===== Status checks

Routers will be periodically queried (by independent goroutines) for:

* Autolink states
* Linkroute states
* Link states

The data will be stored in memory available to the address space and address controller loops.

===== Controllers

The following controllers and components must be implemented:

* Messaging-infra controller - Managing the shared infra
** State representation model - Used by other controllers to apply configuration to shared infra
* Address-space controller - Managing address spaces
* Address controller - Managing addresses
** Address scheduler - Used for placing queues on a set of brokers with different properties/capabilities

==== Tasks

===== Task 1: Create MessagingInfra CRD (small)

* Create the MessaginInfra CRD + OpenAPI.
* Update AddressSpacePlan CRD to allow referencing shared infra in the spec. 
* Address 'shared' type as an allowed address space type.
* Add filter in AddressSpaceController loop on 'brokered' and 'standard'.

==== Task 2: Import router operator definitions (small)

* Add bundle for installing qdrouterd operator in bundle install.
* Add dependency on qdrouterd operator CRDs in OLM manifest.
* (Optional phase 2) Add support for missing podtemplate capabilities.

==== Task 3: Import artemis operator (small/medium)

* Add bundle for installing the broker operator in bundle install
* Add dependency on broker operator CRDs in OLM manifest
* Add support for using init containers to configure broker if needed (if we don't need plugins, we could avoid this step)

==== Task 4: Implement messaging-infra controller in controller-manager (large)

The messaging-infra controller is responsible for managing router and broker deployments and ensure they have the configuration as requested in the config.

The controller should:

* Watch MessagingInfra CR
* Creates router CR to deploy routers based on infra config
* Creates broker CRs to deploy brokers based on infra config and scale on-demand
* Apply to router CR: router <-> broker connectors
* Creates interal state representation for each router and broker in the CR status. This state should be shared with other controllers (details below)

===== Internal state representation

A components state encapsulates the configuration state of a broker or router in memory. Whenever a router or broker is connected, a corresponding router/broker state object is initialized with configuration retrieved from querying the router/broker. If disconnected, the state object is initialized with current state, and desired state is applied.

The state object has methods to apply configuration (i.e. applyAddress, applyAddressSpace). These methods compare the actual configuration of the underlying component to the desired configuration (transformed into autolinks etc.). If the applied configuration is different to the internal state, the underlying component is updated using AMQP management.

In addition ,each state object has a goroutine which periodically polls its underlying router/broker for all status information and caches it for use by controllers to update the address space/address status.

==== Task 5: Implement address-space controller in controller-manager (medium)

The address-space controller manages the AddressSpace CR (filtered by shared infra)

* Watch AddressSpace CR
* Find MessagingInfra where this is placed (based on plan)
* Lookup infra state representation
* For each router:
** Apply vhost settings+limits, external connectors, certs
** Fetch latest known status and update address space status accordingly
* Expose address space metrics based on status
* Requeue address space for processing at configurable interval

==== Task 6: Implement address controller in controller-manager (medium)

* Watch Address CR
* if new address:
** Invoke queue scheduling to configure which brokers address should be placed on
* Find MessagingInfra where this is placed (unless it is 
* Lookup state objects for routers and brokers
* For each router:
** Apply autolinks, linkroutes and addresses
** Fetch latest known status and update address status
* For each broker:
** Apply autolinks, linkroutes and addresses
** Fetch latest known status and update address status
* (Optional phase 2): Expose address metrics based on status
* Requeue address for processing at configurable interval

==== Task 7: Implement queue/topic/subscription scheduling (medium)

The initial version of the queue scheduler should be similar to what we have in the standard address space. It should:

* Allow sharding queues across multiple brokers
* Place addresses on brokers that matches desired semantics
* (Optional phase 2): Take broker anti-affinity into consideration during placement
* (Optional phase 2): Take available broker memory for queue into account during placement

=== Phase 1 VARIANT b

Mostly the same as the above Phase 1 with respect to management of infra, routers and brokers. However, AddressSpace vs Address has been a source of confusion for a long time, and this might be a good time to fix that.

This would mean:

* We remove AddressSpace resource completely and allow only 1 instance of messaging per Kubernetes namespace.
* Create new CRDs for configuring endpoints, connectors etc.

How would one configure an 'address space' without the CRD?! To enable messaging on a namespace, annotations could be used:

```
kubectl annotate namespace myapp enmasse.io/messaging=true
# If a specific plan (i.e. not default should be used)
kubectl annotate namespace myapp enmasse.io/messaging-plan=small
kubectl annotate namespace myapp enmasse.io/authentication-service=standard
```

Then, other parts of the 'address space' with multiple entities, could be configured using CRDs.

To configure connectors:

```
kind: MessagingConnector
matadata:
  name: connector
spec:
  # Same options as under address space .spec.connectors[]?
```

To configure endpoints for messaging:

```
kind: MessagingEndpoint
metadata:
  name: endpoint
spec:
  cert:
    selfsigned: {}
    secret:
      name: endpointcert
    letsencrypt:
      ... #
  ports:
  - amqp
  - amqps
  - amqpws
  - amqpwss
  # Expose as route
  route:
    host: example.com
  # OR loadbalancer
  loadbalancer:
    annotations: {}
status:
  host:
  ports:
  - name: amqp
    port: 5672
```

Instead of reusing AddressSpacePlan, we could create a MessagingPlan with the desired schema sketched already for the AddressSpacePlan.

The main work for shared infra would still be in the messaging-infra and state representations, but there would be no address space controller. Instead, "Task 5" would be modified. In addition, a task for creating the new CRDs would be needed.

==== Task 5: Implement namespace-controller in controller-manager (medium)

The namespace controller watches all namespaces on the cluster for the annotation to enable messaging:

* Watch Namespaces and filter on annotations
* Find MessagingInfra where this is placed (based on plan/default plan)
* Lookup infra state representation
* Lookup MessagingEndpoints in namespace
* Lookup MessagingConnectors in namespace
* For each router:
** Apply vhost settings from plans+limits, external connectors from MessagingConnector, create endpoints and apply certs from MessagingEndpoint
** Fetch latest known status and update status of connectors, endpoints etc.
* Requeue namespace for processing at configurable interval

=== Phase 2 (Milestone 0.33.0 ?)

The second phase will expand the supported features of the shared infra. The shared infra will gain support for deploying broker clusters and assign addresses requiring a broker cluster to them.

After the second phase, the following would be supported as well:

* Connectors and forwarders
* Configure per-address space limits
* Configure per-address limits
* Broker-semantics for addresses - allow 'transactional' address spaces
* Deprecate standard

The following would NOT be supported:
* MQTT, Core, STOMP

=== Phase 3 (Milestone 0.34.0 ?)

* The missing protocol support could be addressed in some way.
* Handle migration from `brokered` and `standard` to `shared`, potentially as part of the enmasse-operator
* Deprecate brokered

=== Phase 4 (Milestone 0.X.0 ?)

Phase 4 would mainly involve removing `brokered` and `standard`, once the oldest version supported in upgrades has deprecated brokered and standard.

* Remove brokered and standard address space types
* Removal of address space `type` field
* Removal of BrokeredInfraConfig and StandardInfraConfig CRDs

== Testing

A new class of tests for shared infra should be created. The address-space-specific tests should be able to reuse the infra to speed up testing. Some tests would still need to be written to test that one can run multiple shared infra instances.

A load-test is also essential to ensure that the operator can handle a large number of address spaces and addresses.

== Documentation

The shared address space will cause a lot of changes to the documentation, and it might be good to create a separate chapter for both service admin and messaging tenant related to shared infra specifically. 
