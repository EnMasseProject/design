[id='sizing-guide-{context}']
= Sizing guide

The sizing guide provides guidelines on how to size {ProductName} installations and what tradeoffs you can expect when adjusting the configuration. Sizing {ProductName} involves configuration of:

* Brokers 
* Routers (standard address space only)
* Operator(s)
* Plans

Each address space type have some distinct features that should be taken into account when creating the address plans. For more information about address space types and their semantics, see link:{BookUrlBase}{BaseProductVersion}{BookNameUrl}#con-address-space-messaging.

NOTE: Sizing {ProductName} components also requires that your {KubePlatform} cluster has sufficient capacity to handle the requested resources. If the {KubePlatform} nodes are configured with 4GB of memory, you cannot configure brokers and routers with memory sizes larger than. Also, as each address space creates a dedicated piece of infrastructure, you need to ensure that cluster capacity can meet demand as the number of address spaces increases. Finally, the use of affinity and tolerations may also restrict the nodes available for the messaging infrastructure to use.

== Sizing brokers

Brokers are configured in the `BrokeredInfraConfig` and `StandardInfraConfig` resources. When sizing a broker, the following must be taken into consideration:

* The average message size
* The number of messages stored
* The number of queues and/or topics
* The address full policy

NOTE: In {ProductName} it is only possible to restrict the total number of memory allocated for a broker, and not restrict memory used by individual addresses.

Enabling address full policy `PAGE` will allow you to store more messages than can fit in memory, at the expense of a potential performance hit from reading data from disk. Therefore, paging is mainly of interest when you have large messages or expect a long backlog of messages in your system.

=== Example

Given 10 queues with a maximum of 1000 messages stored per queue, an average message size of 128kB, the amount of storage space required to store messages is:

```
10 * 1000 * (128 * 1024) = 1.25 GB
```

In addition, the broker has a fixed storage footprint of about 50MB.

The amount of memory required for the broker depends on which address full policy is being used. If the PAGE policy is used, the memory requirements can be reduced by having messages stored on disk rather than in the journal (which always needs to fit in memory). If the FAIL, BLOCK or DROP policies are being used, all messages must be held in memory.

There is also  constant memory cost associated with running the broker as well as the JVM. The memory available to store message are automatically derived from the memory set in the broker configuration and is set to be half the JVM memory, which in turn is set to half of the system memory.

NOTE: In the `standard` address space types, multiple broker instances may get created. The sizing of these also depend on the address plan configuration and how many addresses you expect each broker to be able to handle before another broker is spawned.

==== Example without paging

For non-PAGE policies, an additional 5% bookkeeping overhead per address should be taken into account (`1.05 * 1.25 = 1.35 GB`):

[source,yaml,options="nowrap",subs="+quotes,attributes"]
----
apiVersion: admin.enmasse.io/v1beta1
kind: BrokeredInfraConfig
metadata:
  name: cfg1
spec:
  broker:
    addressFullPolicy: FAIL
    globalMaxSize: 1.35Gb
    resources:
      memory: 4Gi
      storage: 2Gi
  ...
----

==== Example with paging

When paging is enabled, the original formula can be modified to only account for a reference to the message as well as holding 10 in-flight messages in memory:

``` 
(10 * 1000 * 128) + (10 * 128 * 1024) = 2.5MB
```

The amount of memory configured for the broker can now be reduced:

[source,yaml,options="nowrap",subs="+quotes,attributes"]
----
apiVersion: admin.enmasse.io/v1beta1
kind: BrokeredInfraConfig
metadata:
  name: cfg1
spec:
  broker:
    addressFullPolicy: PAGE
    globalMaxSize: 5Mb
    resources:
      memory: 512Mi
      storage: 2Gi
  ...
----

=== Broker scaling (standard address space only)

Brokers are deployed on-demand, i.e. when addresses of type queue or topic are created. The number of brokers is restricted by the resource limits specified in the `AddressSpacePlan`. The following example sets a limit of 4 brokers in total per address space:

----
apiVersion: admin.enmasse.io/v1beta2
kind: AddressSpacePlan
metadata:
  name: cfg1
spec:
  resourceLimits:
    broker: 4.0
  ...
----

In terms of capacity, the memory requirements for the broker should be multiplied by the limit.

The number of broker instances are scaled dynamically between 1 and max limits based on the `AddressPlan` used for the different addresses. An `AddressPlan` describes how big fraction of a broker is required by an address. The fraction defined in the plan is multiplied with the number of addresses referencing this plan and rounded upwards to produce the number of desired broker replicas. 

Example:
----
apiVersion: admin.enmasse.io/v1beta2
kind: AddressPlan
metadata:
  name: plan1
spec:
  ...
  resources:
    broker: 0.01
----

If you create 110 addresses with `plan1` as the plan, the number of broker replicas will be `ceil(110 * 0.01) = 2 replicas`. 

The total number of brokers will capped by the address space plan resource limits.

== Routers

Routers are configured in the `StandardInfraConfig` resource. The router sizing must take the following into account:

* The number of addresses
* The number of connections and links
* Link capacity

The router does not persist any state and therefore does not require persistent storage.

Address configuration itself does not require significant amount of router memory. However, queues and subscriptions require an additional 2 links between the router and broker per address.

The total number of links is then the number of queues/subscriptions + the number of client links. Each link requires metadata and buffers in the router to deal with routing messages for that link.

The router link capacity affects how many in-flight messages the router will handle per link. Setting this to a high value could improve performance, but at the cost of potentially more memory being used to hold in-flight messages. If you have low-volume client traffic, using the default link capacity should be sufficient.

=== Example

Sizing should accomodate 500 anycast and 1000 queued addresses, with 10000 connected clients (1 link each), with a link capacity of 100 (max number of in-flight messages per link), and an average message size of 512 bytes.

Based on measurements, an estimated 20kB overhead per anycast address is realistic:
[options="nowrap",subs="+quotes,attributes"]
----
500 * 20kB = 10MB
----

Memory usage of queues and topics is slightly higher than for anycast addresses, with a 40kB overhead per address. In addition, each link may have up to `linkCapacity` messages in flight:
[options="nowrap",subs="+quotes,attributes"]
----
(1000 * 40kB) + (2000 * 100 * 512) = 135MB
----

Memory usage of client connections/links:
[options="nowrap",subs="+quotes,attributes"]
----
10000 * 100 * 512 = 488MB
----

The total amount of router memory required for this configuration (including a constant base memory of 50MB) is `10 + 135 + 488 + 50 = 683MB`. 

In order to ensure max connections and links is not exceeded, a router policy can be applied as well. The router config looks at follows:

[source,yaml,options="nowrap",subs="+quotes,attributes"]
----
apiVersion: admin.enmasse.io/v1beta1
kind: StandardInfraConfig 
metadata:
  name: cfg1
spec:
  router:
    resources:
      memory: 700Mi
    linkCapacity: 100
    policy:
      maxConnections: 10000
      maxSessionsPerConnection: 1
      maxSendersPerConnection: 1
      maxReciversPerConnection: 1
  ...
----

=== High Availability

Configuring routes for HA (High Availability) means you need to multiply the minimum required router replicas by the memory per router to get the expected memory usage.

=== Router scaling

Routers are scaled dynamically on demand within the interval of `minReplicas` defined in the `StandardInfraConfig` resource and `resourceLimits.router` defined in the `AddressSpacePlan`. To restrict the number of routers to max four, but requiring a minimum amount of 2 routers for HA purposes, the following configuration is needed:

----
apiVersion: admin.enmasse.io/v1beta1
kind: StandardInfraConfig 
metadata:
  name: cfg1
spec:
  router:
    minReplicas: 2
  ...
---
apiVersion: admin.enmasse.io/v1beta2
kind: AddressSpacePlan
metadata:
  name: plan1
spec:
  infraConfigRef: cfg1
  resourceLimits:
    router: 4
  ...
----

In terms of capacity, the memory requirements for the router should be multiplied by the resource limit. The router will scale up to the resource limits defined in the `AddressSpacePlan` for the address space.

The number of router replicas are scaled dynamically between the min and max limits based on the `AddressPlan` used for the different addresses. An `AddressPlan` describes how big fraction of a router is required by an address. The fraction defined in the plan is multiplied with the number of addresses referencing this plan and rounded upwards to produce the number of desired router replicas. 

Example:
----
apiVersion: admin.enmasse.io/v1beta2
kind: AddressPlan
metadata:
  name: plan1
spec:
  ...
  resources:
    router: 0.01
----

If you create 110 addresses with `plan1` as the plan, the number of router replicas will be `ceil(110 * 0.01) = 2 replicas`. 

If the amount of replicas go above the address space plan limit, the addresses exceeding the max will remain in the `Pending` state and an error message describing the issue will be set in the `Address` status section.

== Operators

Operators are tasked with reading all address configuration and applying this to the routers and brokers. The operators should be sized proportionally to the number of addresses.

In the `standard` address space, the operator pod contains two processes, `agent` and `standard-controller`. These cannot be sized individually, but memory usage of both are proportional to the number of addresses. In the `brokered` space, there is only a single `agent` process.

NOTE: the operator processes are running either on a JVM or a Node.JS VM. The memory for these should be sized twice the amount of memory required for the address configuration itself.

=== Example

Each address adds about 20kB overhead to the operator process. With 1500 addresses, an additional `1500 * 2kB = 30MB` is needed for the operator process.

In addition, there is a base memory requirement of 256MB for these processes, making the total operator memory `256 + 30 = 286 MB`. This can be configured in both the `StandardInfraConfig` and `BrokeredInfraConfig` resources:
[source,yaml,options="nowrap",subs="+quotes,attributes"]
----
apiVersion: admin.enmasse.io/v1beta1
kind: StandardInfraConfig 
metadata:
  name: cfg1
spec:
  admin:
    resources:
      memory: 300Mi 
  ...
----


== Plans

Plans enable dynamic scaling in the `standard` address space, as shown in the broker and router sizing sections. At the cluster level, the combination of plans and infrastructure configs will determine the max number of pods that can be deployed on the cluster. At present, {ProductName} does not support limiting the number of address spaces that gets created, so some policy on who is allowed to create them should be applied. This can be handled through standard {KubePlatform} policies.

From a capacity planning perspective, it is useful to understand what the maximum number of pods and memory can be consumed for a given address space.

.Procedure

. Save the following script as `check-memory.sh`
+
NOTE: Memory is assumed to be defined using the 'Mi' unit, while storage is assumed to be defined using the 'Gi' unit. All of `admin`, `router` and `broker` must have limits set in order for the script to work as expected.
+
[source,shell,options="nowrap",subs="+quotes,attributes"]
----
#!/usr/bin/env bash
PLAN=$1

total_pods=0
total_memory_mb=0
total_storage_gb=0

routers=$(oc get addressspaceplan $PLAN -o jsonpath='{.spec.resourceLimits.router}')
brokers=$(oc get addressspaceplan $PLAN -o jsonpath='{.spec.resourceLimits.broker}')
infra=$(oc get addressspaceplan $PLAN -o jsonpath='{.spec.infraConfigRef}')

operator_memory=$(oc get standardinfraconfig $infra -o jsonpath='{.spec.admin.resources.memory}')
broker_memory=$(oc get standardinfraconfig $infra -o jsonpath='{.spec.broker.resources.memory}')
broker_storage=$(oc get standardinfraconfig $infra -o jsonpath='{.spec.broker.resources.storage}')
router_memory=$(oc get standardinfraconfig $infra -o jsonpath='{.spec.router.resources.memory}')

total_pods=$((routers + brokers + 1))
total_memory_mb=$(( (routers * ${router_memory%Mi}) + (brokers * ${broker_memory%Mi}) + ${operator_memory%Mi}))
total_storage_gb=$(( brokers * ${broker_storage%Gi}))

echo "Pods: ${total_pods}. Memory: ${total_memory_mb} MB. Storage: ${total_storage_gb} GB"
----

. Run it as follows:
+
----
bash calculate-memory.sh _standard-small_
----

. If all components have limits defined in the assumed units, it will output the total resource limits for address spaces using this plan:
+
----
Pods: 3. Memory: 1280 MB. Storage: 2 GB
----